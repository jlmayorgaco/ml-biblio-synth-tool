{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/jorge.mayorga/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/jorge.mayorga/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jorge.mayorga/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### import importlib\n",
    "import sys\n",
    "import os\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Add the 'src' directory to the system path\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "src_path = os.path.abspath('../src')\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import classes from the modules using their correct filenames\n",
    "from DataLoaderClass import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize paths\n",
    "BIB_FILE_PATH = '../examples/EX1_POWER_SYSTEM_FPGA_FREQUENCY_ESTIMATORS/index.bib'\n",
    "PDF_FOLDER_PATH = '../examples/EX1_POWER_SYSTEM_FPGA_FREQUENCY_ESTIMATORS/files'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matching process completed.\n",
      "Total references matched: 31 out of 31\n",
      "Unmatched References: 0\n",
      "\n",
      "Unmatched PDF Folders: 0\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Data Loading and Processing\n",
    "loader = DataLoader(BIB_FILE_PATH, PDF_FOLDER_PATH)\n",
    "processed_data = loader.load_and_process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------- #\n",
    "# -- EDA M4 :: Quotes & Cites ---------------------------------- #\n",
    "# -------------------------------------------------------------- #\n",
    "from eda.m4_quotes_analysis import Processor\n",
    "from eda.m4_quotes_analysis import Visualizer\n",
    "from eda.m4_quotes_analysis import Reporter\n",
    "# -------------------------------------------------------------- #\n",
    "\n",
    "# Data\n",
    "data = processed_data\n",
    "\n",
    "# Process data\n",
    "processor = Processor(data)\n",
    "visualizer = Visualizer()\n",
    "reporter = Reporter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "### Function 1 => Most Frequent Quotes (Table & Barplot)\n",
    "#################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Quote belongs to Cluster 1\n",
      "Similar Quotes in this Cluster: ['xxxiiipart', '1 machine rotor speeds 2278 2 center of inertia 2478 3 applications of the rocop 2569 power system model 2619 1 introduction', 'i e v v θ v', 'i e v v jvdq', 'ϑ t 1 4 dt dt2to properly interpret 1 2 is not always trivial if the angular frequency isconstant the definition is also consistent with the intuition in fact if ϑ is ϑ t ω t θ 1 5 o owhere ω']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Load spaCy language model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Define the text cleaning function\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean the text by removing unwanted characters or patterns.\n",
    "    \"\"\"\n",
    "    import re\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces/newlines with a single space\n",
    "    text = re.sub(r'\\W+', ' ', text)  # Remove non-alphanumeric characters\n",
    "    return text.strip()\n",
    "\n",
    "# Function to extract quotes using spaCy\n",
    "def extract_quotes_spacy(text, min_length=10, max_length=300):\n",
    "    \"\"\"\n",
    "    Extract quotes using spaCy's sentence segmentation and filter by length.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    quotes = []\n",
    "    for sent in doc.sents:\n",
    "        sentence_text = sent.text.strip()\n",
    "        if min_length <= len(sentence_text) <= max_length:\n",
    "            quotes.append(sentence_text)\n",
    "    return quotes\n",
    "\n",
    "# Function to cluster sentences and save the model\n",
    "def do_cluster_sentences(sentences, n_clusters=5, save_model=True):\n",
    "    \"\"\"\n",
    "    Cluster sentences into n_clusters using TF-IDF and KMeans. Save the model if required.\n",
    "    \"\"\"\n",
    "    if not sentences:\n",
    "        raise ValueError(\"The sentences list is empty. Provide valid input.\")\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(stop_words='english', max_features=500)\n",
    "    X = vectorizer.fit_transform(sentences)\n",
    "    \n",
    "    model = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    model.fit(X)\n",
    "    \n",
    "    # Save the model and vectorizer\n",
    "    if save_model:\n",
    "        with open(\"tfidf_vectorizer.pkl\", \"wb\") as f:\n",
    "            pickle.dump(vectorizer, f)\n",
    "        with open(\"kmeans_model.pkl\", \"wb\") as f:\n",
    "            pickle.dump(model, f)\n",
    "    \n",
    "    clusters = {i: [] for i in range(n_clusters)}\n",
    "    for idx, label in enumerate(model.labels_):\n",
    "        clusters[label].append(sentences[idx])\n",
    "    \n",
    "    return clusters, model, vectorizer\n",
    "\n",
    "# Function to predict the cluster for a new quote\n",
    "def predict_quote_cluster(quote, model, vectorizer):\n",
    "    \"\"\"\n",
    "    Predict the cluster for a new quote using the trained model and vectorizer.\n",
    "    \"\"\"\n",
    "    quote_vector = vectorizer.transform([quote])\n",
    "    cluster_id = model.predict(quote_vector)[0]\n",
    "    return cluster_id\n",
    "\n",
    "# Example Usage\n",
    "all_quotes = []\n",
    "\n",
    "# Extract quotes\n",
    "for entry in processed_data[:5]:  # Analyze the first 5 entries\n",
    "    raw_text = entry.get(\"plain_text\", \"\")\n",
    "    cleaned_text = clean_text(raw_text)\n",
    "    quotes = extract_quotes_spacy(cleaned_text)\n",
    "    all_quotes.extend(quotes)\n",
    "\n",
    "# Perform clustering and save results\n",
    "clusters, kmeans_model, tfidf_vectorizer = do_cluster_sentences(all_quotes, n_clusters=3)\n",
    "\n",
    "# Save clusters to a CSV file\n",
    "csv_data = []\n",
    "for cluster_id, cluster_sentences in clusters.items():\n",
    "    for sentence in cluster_sentences:\n",
    "        csv_data.append({\"Cluster\": cluster_id, \"Quote\": sentence})\n",
    "\n",
    "df = pd.DataFrame(csv_data)\n",
    "df.to_csv(\"clusters.csv\", index=False)\n",
    "\n",
    "# Predict cluster for a new quote\n",
    "new_quote = \"FPGA Kalman\"\n",
    "predicted_cluster = predict_quote_cluster(new_quote, kmeans_model, tfidf_vectorizer)\n",
    "similar_quotes = clusters[predicted_cluster]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " new_quote \n",
      "The frequency variations in power systems require dynamic control.\n",
      " \n",
      "New Quote belongs to Cluster 1\n",
      "Similar Quotes in this Cluster: ['xxxiiipart', '1 machine rotor speeds 2278 2 center of inertia 2478 3 applications of the rocop 2569 power system model 2619 1 introduction', 'i e v v θ v', 'i e v v jvdq', 'ϑ t 1 4 dt dt2to properly interpret 1 2 is not always trivial if the angular frequency isconstant the definition is also consistent with the intuition in fact if ϑ is ϑ t ω t θ 1 5 o owhere ω']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\" \")\n",
    "print(\" new_quote \")\n",
    "print(new_quote)\n",
    "print(\" \")\n",
    "print(f\"New Quote belongs to Cluster {predicted_cluster}\")\n",
    "print(f\"Similar Quotes in this Cluster: {similar_quotes[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "### Function 2 => Quotes by Year (Table & Line Chart)\n",
    "#################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Function 2: Quotes by Year**\n",
    "\n",
    "#### **Overview**:\n",
    "This analysis examines how quotes are distributed over time, providing insights into the evolution of popular ideas and recurring themes in the dataset. By grouping quotes by year and counting their occurrences, we can track the rise or decline of specific quotes.\n",
    "\n",
    "#### **Interpretation**:\n",
    "- **Emerging Quotes**: Quotes that appear frequently in recent years may indicate emerging ideas or trends in the field.\n",
    "- **Declining Quotes**: Quotes that were more common in earlier years but less so now might represent outdated concepts or fading interest.\n",
    "- **Consistent Quotes**: Quotes with steady usage across years may highlight foundational or widely accepted principles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Quote'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Step 1: Analyze quotes by year\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m quotes_by_year \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_quotes_by_year\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Step 2: Visualize the trends of quotes over time\u001b[39;00m\n\u001b[1;32m      5\u001b[0m Visualizer\u001b[38;5;241m.\u001b[39mplot_quote_trends(\n\u001b[1;32m      6\u001b[0m     df\u001b[38;5;241m=\u001b[39mquotes_by_year,\n\u001b[1;32m      7\u001b[0m     title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuote Trends Over Time\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m     filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquote_trends\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/GitHub/ml-biblio-synth-tool/src/eda/m4_quotes_analysis.py:105\u001b[0m, in \u001b[0;36mProcessor.get_quotes_by_year\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    102\u001b[0m quotes_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(quote_counts)\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# Group by quote and year to count occurrences\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m grouped \u001b[38;5;241m=\u001b[39m \u001b[43mquotes_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mQuote\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYear\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;241m.\u001b[39mreset_index(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m grouped\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ML-Master/lib/python3.8/site-packages/pandas/core/frame.py:8252\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   8249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to supply one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mby\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   8250\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis_number(axis)\n\u001b[0;32m-> 8252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameGroupBy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   8253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8255\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mas_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8258\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8259\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8262\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ML-Master/lib/python3.8/site-packages/pandas/core/groupby/groupby.py:931\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropna \u001b[38;5;241m=\u001b[39m dropna\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     grouper, exclusions, obj \u001b[38;5;241m=\u001b[39m \u001b[43mget_grouper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    933\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    936\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;241m=\u001b[39m obj\n\u001b[1;32m    942\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_get_axis_number(axis)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ML-Master/lib/python3.8/site-packages/pandas/core/groupby/grouper.py:985\u001b[0m, in \u001b[0;36mget_grouper\u001b[0;34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[0m\n\u001b[1;32m    983\u001b[0m         in_axis, level, gpr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    984\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 985\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[1;32m    986\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    987\u001b[0m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[1;32m    988\u001b[0m     exclusions\u001b[38;5;241m.\u001b[39madd(gpr\u001b[38;5;241m.\u001b[39mkey)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Quote'"
     ]
    }
   ],
   "source": [
    "# Step 1: Analyze quotes by year\n",
    "quotes_by_year = processor.get_quotes_by_year()\n",
    "\n",
    "# Step 2: Visualize the trends of quotes over time\n",
    "Visualizer.plot_quote_trends(\n",
    "    df=quotes_by_year,\n",
    "    title=\"Quote Trends Over Time\",\n",
    "    filename=\"quote_trends\"\n",
    ")\n",
    "\n",
    "# Step 3: Save the table\n",
    "Reporter.save_to_csv(quotes_by_year, \"quotes_by_year\")\n",
    "\n",
    "# Step 4: Display the table\n",
    "quotes_by_year.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "### Function 3 => Quotes Context Analysis (Table)\n",
    "#################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Function 3: Quotes Context Analysis**\n",
    "\n",
    "#### **Overview**:\n",
    "This analysis extracts the context in which specific quotes appear, including sentences or paragraphs before and after the quote. It helps understand how quotes are used and their relevance to the surrounding discussion.\n",
    "\n",
    "#### **Interpretation**:\n",
    "- **Contextual Insights**: Analyze the sentences around a quote to better understand its role in the text.\n",
    "- **Relevance**: Identify how quotes are used to support arguments or highlight key ideas.\n",
    "- **Metadata**: Connect quotes to their associated metadata, such as title, authors, and year, for a richer understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Step 1: Define quotes to analyze\n",
    "quotes_to_analyze = [\n",
    "    \"Frequency is a challenging parameter to estimate.\",\n",
    "    \"Power systems require stability and robustness.\"\n",
    "]\n",
    "\n",
    "# Step 2: Extract contexts for selected quotes\n",
    "quote_contexts = processor.get_quote_contexts(quotes_to_analyze, context_window=2)\n",
    "\n",
    "# Step 3: Save the table\n",
    "Reporter.save_to_csv(quote_contexts, \"quote_contexts\")\n",
    "\n",
    "# Step 4: Display the table\n",
    "quote_contexts.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "### Sentiment Analysis of Quote Contexts\n",
    "#################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Extract contexts for selected quotes\n",
    "contexts = processor.get_quote_contexts(\n",
    "    quotes_to_analyze=[\"Frequency is a challenging parameter to estimate.\"],\n",
    "    context_window=2\n",
    ")\n",
    "\n",
    "# Step 2: Analyze sentiment of the contexts\n",
    "sentiment_df = processor.analyze_sentiment_of_contexts(contexts)\n",
    "\n",
    "# Step 3: Visualize sentiment distribution\n",
    "Visualizer.plot_sentiment_distribution(\n",
    "    df=sentiment_df,\n",
    "    title=\"Sentiment Distribution of Quote Contexts\",\n",
    "    filename=\"quote_sentiment_distribution\"\n",
    ")\n",
    "\n",
    "# Step 4: Save the table\n",
    "Reporter.save_to_csv(sentiment_df, \"quote_sentiment_analysis\")\n",
    "\n",
    "# Step 5: Display the sentiment analysis table\n",
    "sentiment_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "### Theme Extraction from Quote Contexts\n",
    "#################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Extract contexts for selected quotes\n",
    "contexts = processor.get_quote_contexts(\n",
    "    quotes_to_analyze=[\"Frequency is a challenging parameter to estimate.\"],\n",
    "    context_window=2\n",
    ")\n",
    "\n",
    "# Step 2: Extract themes (keywords) from the contexts\n",
    "themes_df = processor.extract_themes_from_contexts(contexts, top_n=10)\n",
    "\n",
    "# Step 3: Visualize themes with a word cloud\n",
    "Visualizer.plot_word_cloud_from_themes(\n",
    "    df=themes_df,\n",
    "    title=\"Themes from Quote Contexts\",\n",
    "    filename=\"quote_context_themes\"\n",
    ")\n",
    "\n",
    "# Step 4: Save the table\n",
    "Reporter.save_to_csv(themes_df, \"quote_context_themes\")\n",
    "\n",
    "# Step 5: Display the themes table\n",
    "themes_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "themes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
