{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/jorge.mayorga/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jorge.mayorga/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/jorge.mayorga/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/jorge.mayorga/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/jorge.mayorga/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/jorge.mayorga/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jorge.mayorga/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### import importlib\n",
    "import spacy\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "english_vocab = set(words.words())\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Add the 'src' directory to the system path\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "src_path = os.path.abspath('../src')\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import classes from the modules using their correct filenames\n",
    "from DataLoaderClass import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize paths\n",
    "BIB_FILE_PATH = '../examples/EX2_POWER_SYSTEM_FPGA_FREQUENCY_ESTIMATORS/index.bib'\n",
    "PDF_FOLDER_PATH = '../examples/EX2_POWER_SYSTEM_FPGA_FREQUENCY_ESTIMATORS/files'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matching process completed.\n",
      "Total references matched: 53 out of 53\n",
      "Unmatched References: 0\n",
      "\n",
      "Unmatched PDF Folders: 0\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Data Loading and Processing\n",
    "loader = DataLoader(BIB_FILE_PATH, PDF_FOLDER_PATH)\n",
    "processed_data = loader.load_and_process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------- #\n",
    "# -- EDA M4 :: Quotes & Cites ---------------------------------- #\n",
    "# -------------------------------------------------------------- #\n",
    "from eda.m4_quotes_analysis import Processor\n",
    "from eda.m4_quotes_analysis import Visualizer\n",
    "from eda.m4_quotes_analysis import Reporter\n",
    "# -------------------------------------------------------------- #\n",
    "\n",
    "# Data\n",
    "data = processed_data\n",
    "\n",
    "# Process data\n",
    "processor = Processor(data)\n",
    "visualizer = Visualizer()\n",
    "reporter = Reporter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "### Function 1 => Most Frequent Quotes (Table & Barplot)\n",
    "#################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_long_word_with_vocab(word, vocabulary):\n",
    "    \"\"\"\n",
    "    Split a long word into subwords using a predefined English vocabulary.\n",
    "    \"\"\"\n",
    "    subwords = []\n",
    "    current = word.lower()\n",
    "    while current:\n",
    "        match = None\n",
    "        for i in range(len(current), 0, -1):\n",
    "            substring = current[:i]\n",
    "            if substring in vocabulary:\n",
    "                match = substring\n",
    "                break\n",
    "        if match:\n",
    "            subwords.append(match)\n",
    "            current = current[len(match):]\n",
    "        else:\n",
    "            subwords.append(current)  # Keep as is if no match\n",
    "            break\n",
    "    return subwords\n",
    "\n",
    "def clean_and_split_text_with_vocab(plain_text, vocabulary):\n",
    "    \"\"\"\n",
    "    Clean and split text using vocabulary-based splitting.\n",
    "    \"\"\"\n",
    "    # Lowercase and remove non-alphabetic characters\n",
    "    plain_text = re.sub(r'[^a-zA-Z\\s]', ' ', plain_text.lower())\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(plain_text)\n",
    "\n",
    "    # Remove stop words and single characters\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words and len(word) > 1]\n",
    "\n",
    "    # Split concatenated words using the vocabulary\n",
    "    split_tokens = []\n",
    "    for token in tokens:\n",
    "        if len(token) > 10:  # Treat long words as concatenated\n",
    "            split_tokens.extend(split_long_word_with_vocab(token, vocabulary))\n",
    "        else:\n",
    "            split_tokens.append(token)\n",
    "\n",
    "    # Lemmatize the words with SpaCy\n",
    "    lemmatized_words = []\n",
    "    for token in nlp(\" \".join(split_tokens)):\n",
    "        if token.is_alpha and not token.is_stop:\n",
    "            lemmatized_words.append(token.lemma_)\n",
    "\n",
    "    # Remove domain-specific noise words\n",
    "    noise_words = {\"doi\", \"org\", \"http\", \"https\", \"www\"}\n",
    "    lemmatized_words = [word for word in lemmatized_words if word not in noise_words]\n",
    "    \n",
    "    return lemmatized_words\n",
    "\n",
    "def process_document(doc, vocabulary):\n",
    "    \"\"\"\n",
    "    Process a single document:\n",
    "    - Cleans and tokenizes plain_text.\n",
    "    - Splits authors into a list.\n",
    "    \"\"\"\n",
    "    # Extract and clean plain_text\n",
    "    plain_text = doc.get('plain_text', '')\n",
    "    words = clean_and_split_text_with_vocab(plain_text, vocabulary)\n",
    "\n",
    "    # Split authors into a list\n",
    "    authors_raw = doc.get('bibliographic_metadata', {}).get('author', '')\n",
    "    authors = [author.strip() for part in authors_raw.split(\",\") for author in part.split(\" and \")]\n",
    "\n",
    "    # Add the processed data to the document\n",
    "    doc['words'] = words\n",
    "    doc['authors'] = authors\n",
    "\n",
    "    return doc\n",
    "\n",
    "\n",
    "def process_documents(data, vocabulary):\n",
    "    \"\"\"\n",
    "    Process a list of documents using the vocabulary.\n",
    "    \"\"\"\n",
    "    return [process_document(doc, vocabulary) for doc in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the documents\n",
    "no_one_letter_english_vocab = [word for word in english_vocab if len(word) > 1]\n",
    "processed_data = process_documents(data, no_one_letter_english_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_metadata(docs):\n",
    "    \"\"\"\n",
    "    Flatten the bibliographic_metadata into individual columns for each document.\n",
    "    \"\"\"\n",
    "    flattened_docs = []\n",
    "    for doc in docs:\n",
    "        # Extract bibliographic_metadata and flatten it\n",
    "        metadata = doc.pop(\"bibliographic_metadata\", {})\n",
    "        flattened_doc = {**doc, **metadata}  # Merge metadata into the main doc\n",
    "        flattened_docs.append(flattened_doc)\n",
    "    return flattened_docs\n",
    "\n",
    "# Flatten the metadata in processed_data\n",
    "flattened_data = flatten_metadata(processed_data)\n",
    "\n",
    "# Convert to a pandas DataFrame for tabular representation\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(flattened_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use your DataFrame\n",
    "df = pd.DataFrame(flattened_data)\n",
    "\n",
    "# Explode authors and words into individual rows\n",
    "exploded_df = df.explode(\"authors\").explode(\"words\")\n",
    "\n",
    "# Count co-occurrences of authors and words\n",
    "co_occurrence_counts = exploded_df.groupby([\"authors\", \"words\"]).size().reset_index(name=\"count\")\n",
    "\n",
    "# Pivot to create a co-occurrence matrix\n",
    "co_occurrence_matrix = co_occurrence_counts.pivot_table(\n",
    "    index=\"authors\", columns=\"words\", values=\"count\", fill_value=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommend related words based on co-occurrence\n",
    "def recommend_words(author_name, input_word, co_occurrence_matrix, top_n=5):\n",
    "    \"\"\"\n",
    "    Recommend words related to an input word for a given author.\n",
    "    \n",
    "    Args:\n",
    "        author_name (str): Name of the author.\n",
    "        input_word (str): The word for which to find related words.\n",
    "        co_occurrence_matrix (pd.DataFrame): Co-occurrence matrix.\n",
    "        top_n (int): Number of related words to return.\n",
    "        \n",
    "    Returns:\n",
    "        pd.Series: Top related words with their similarity scores.\n",
    "    \"\"\"\n",
    "    # Ensure the author exists\n",
    "    if author_name not in co_occurrence_matrix.index:\n",
    "        raise ValueError(f\"Author '{author_name}' not found in co-occurrence matrix.\")\n",
    "    \n",
    "    # Ensure the word exists\n",
    "    if input_word not in co_occurrence_matrix.columns:\n",
    "        raise ValueError(f\"Word '{input_word}' not found in co-occurrence matrix.\")\n",
    "    \n",
    "    # Get the vector for the author's words\n",
    "    author_vector = co_occurrence_matrix.loc[author_name].values.reshape(1, -1)\n",
    "    \n",
    "    # Compute cosine similarity for all words\n",
    "    word_matrix = co_occurrence_matrix.T.values  # Transpose for word-based similarity\n",
    "    similarity_scores = cosine_similarity(author_vector, word_matrix).flatten()\n",
    "    \n",
    "    # Create a DataFrame of words and their similarity scores\n",
    "    words = co_occurrence_matrix.columns\n",
    "    related_words = pd.Series(similarity_scores, index=words).sort_values(ascending=False)\n",
    "    \n",
    "    # Exclude the input word itself and return the top N related words\n",
    "    return related_words.drop(index=input_word).head(top_n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "author_name = \"Chughtai\"\n",
    "input_word = \"frequency\"\n",
    "recommended_words = recommend_words(author_name, input_word, co_occurrence_matrix)\n",
    "print(recommended_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the matrix into a DataFrame for training\n",
    "X = co_occurrence_matrix\n",
    "y = co_occurrence_matrix.idxmax(axis=1)  # Simplified target example\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Random Forest Classifier\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Test the model\n",
    "score = clf.score(X_test, y_test)\n",
    "print(f\"Model Accuracy: {score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Compute similarity between authors based on the co-occurrence matrix\n",
    "author_similarity = cosine_similarity(co_occurrence_matrix)\n",
    "\n",
    "# Recommend words for an author\n",
    "def recommend_words(author_name, top_n=5):\n",
    "    author_idx = co_occurrence_matrix.index.get_loc(author_name)\n",
    "    similar_authors = np.argsort(-author_similarity[author_idx])  # Descending order\n",
    "    recommended_words = co_occurrence_matrix.iloc[similar_authors[:top_n]].sum(axis=0)\n",
    "    return recommended_words.sort_values(ascending=False).head(top_n)\n",
    "\n",
    "# Example usage\n",
    "print(recommend_words(\"Chughtai\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
